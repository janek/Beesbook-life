{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mi/rrszynka/mnt/janek/Beesbook-life/Analysis\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'create_presence_locations_cam_cache_filename'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3ae4cc7a5548>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../Python-modules/'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#For bee_helpers and file_helpers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfile_helpers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_presence_locations_cam_cache_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_presence_cache_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_presence_locations_cache_filename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbee_helpers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcalc_trip_starts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'create_presence_locations_cam_cache_filename'"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import math\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.stats import variation\n",
    "import sys \n",
    "sys.path.append('../Python-modules/') #For bee_helpers and file_helpers \n",
    "from file_helpers import create_presence_locations_cam_cache_filename, create_presence_cache_filename, create_presence_locations_cache_filename\n",
    "from bee_helpers import calc_trip_starts\n",
    "\n",
    "#from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from skimage.filters import threshold_minimum, threshold_mean\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "from bb_utils.ids import BeesbookID\n",
    "from bb_utils import meta, ids\n",
    "#from data_utils.visualization import bivariate_kdeplot\n",
    "import psycopg2\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connect_str = \"\"\"dbname='beesbook' \n",
    "                 user='postgres' \n",
    "                 host='tonic.imp.fu-berlin.de' \n",
    "                 password='' \n",
    "                 application_name='ben_bee_features'\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bb_utils import meta\n",
    "from datetime import timedelta, datetime\n",
    "from tqdm import tqdm_notebook\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_weeks = 8\n",
    "num_days = 7 * num_weeks\n",
    "days_after = 7 * 2\n",
    "\n",
    "group_idx = 25\n",
    "\n",
    "reference_date = meta.BeeMetaInfo().get_foragergroup(group_idx).date\n",
    "start_date = reference_date + timedelta(days_after) - timedelta(num_days)\n",
    "end_date = reference_date + timedelta(days_after)\n",
    "\n",
    "cache_location_prefix = \"../../caches/\"\n",
    "detections_cache_path = cache_location_prefix + \"Detections/\"\n",
    "num_days_to_process = 2\n",
    "\n",
    "assert (end_date < datetime(2016, 9, 19))\n",
    "assert (start_date > datetime(2016, 7, 19))\n",
    "\n",
    "start_date, reference_date + timedelta(days_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tnrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_compute_job(day):\n",
    "    class TrackFeatures:\n",
    "        def __init__(self, ts_begin, ts_end, confidence=.95, min_track_len=3):\n",
    "            self.ts_begin = ts_begin\n",
    "            self.ts_end = ts_end\n",
    "            self.confidence = confidence\n",
    "            self.min_track_len = min_track_len\n",
    "            self.meta = meta.BeeMetaInfo()\n",
    "\n",
    "            self.exit_positions = {\n",
    "                0: np.array([6000 - 50, 280]),\n",
    "                1: np.array([3950, 280]),\n",
    "                2: np.array([250, 300]),\n",
    "                3: np.array([6000 - 250, 4000 - 300])\n",
    "            }\n",
    "\n",
    "            self.prefix_query = \"\"\"\n",
    "            SET geqo_effort to 10;\n",
    "            SET max_parallel_workers_per_gather TO 8;\n",
    "            SET temp_buffers to \"32GB\";\n",
    "            SET work_mem to \"1GB\";\n",
    "            SET temp_tablespaces to \"ssdspace\";\n",
    "            \"\"\"\n",
    "\n",
    "        @staticmethod\n",
    "        def process_group(group, min_track_len, exit_positions):\n",
    "            if len(group) < min_track_len:\n",
    "                return None\n",
    "\n",
    "            exit_pos = exit_positions[group.cam_id.unique()[0]]\n",
    "            group = group.sort_values('timestamp', ascending=True)\n",
    "\n",
    "            bee_id = group.bee_id.unique()[0]\n",
    "            dt = group['timestamp'].diff()\n",
    "            valid = dt.apply(lambda t: t.total_seconds()).values[1:] < (.3 * 2)\n",
    "\n",
    "            dv = group[['x_pos', 'y_pos']].diff()[1:].apply(\n",
    "                np.linalg.norm, axis=1).values\n",
    "            valid = np.logical_and(valid, dv < np.expm1(6))\n",
    "            dv = dv[valid]\n",
    "            hd = np.linalg.norm(\n",
    "                group[['x_pos', 'y_pos']].values - exit_pos[None, :], axis=1)\n",
    "\n",
    "            return bee_id, dv, hd\n",
    "\n",
    "        def get_data(self, conn):\n",
    "            data_query = \"\"\"\n",
    "                SELECT q.* \n",
    "                FROM (\n",
    "                    SELECT * from bb_detections_2016_stitched\n",
    "                    WHERE timestamp >= %s\n",
    "                        AND timestamp < %s\n",
    "                        AND bee_id_confidence > %s\n",
    "                        AND MOD(track_id, 4) = 0\n",
    "                ) q\n",
    "                INNER JOIN alive_bees_2016 \n",
    "                ON (alive_bees_2016.bee_id = q.bee_id) AND\n",
    "                   (EXTRACT(doy FROM alive_bees_2016.timestamp) = EXTRACT(doy FROM q.timestamp));\n",
    "            \"\"\"\n",
    "            feature_query = \"\"\"\n",
    "                SELECT *, oid as combined_id\n",
    "                INTO TEMPORARY TABLE frame_ids\n",
    "                FROM matching_frames_2016\n",
    "                WHERE timestamp_a >= %s\n",
    "                    AND timestamp_a < %s\n",
    "                ORDER BY RANDOM() \n",
    "                LIMIT 2000;\n",
    "\n",
    "                SELECT q.*\n",
    "                INTO TEMPORARY TABLE detections\n",
    "                FROM (\n",
    "                    SELECT frame_id, bee_id, x_pos_hive, y_pos_hive, timestamp, cam_id\n",
    "                    FROM bb_detections_2016_stitched\n",
    "                    WHERE frame_id in (SELECT frame_id_a FROM frame_ids UNION\n",
    "                        SELECT frame_id_b FROM frame_ids)\n",
    "                        AND bee_id_confidence > .95\n",
    "                ) q\n",
    "                INNER JOIN alive_bees_2016 \n",
    "                ON (alive_bees_2016.bee_id = q.bee_id) AND\n",
    "                   (EXTRACT(doy FROM alive_bees_2016.timestamp) = EXTRACT(doy FROM q.timestamp));\n",
    "\n",
    "                SELECT * \n",
    "                INTO TEMPORARY TABLE detections_with_combined_id\n",
    "                FROM (\n",
    "                    SELECT *\n",
    "                    FROM detections A\n",
    "                    INNER JOIN frame_ids B ON A.frame_id = B.frame_id_a\n",
    "                    UNION \n",
    "                    SELECT *\n",
    "                    FROM detections A\n",
    "                    INNER JOIN frame_ids B ON A.frame_id = B.frame_id_b\n",
    "                ) q;\n",
    "\n",
    "                SELECT A.bee_id as bee_id_a,\n",
    "                    B.bee_id as bee_id_b,\n",
    "                    (|/((A.x_pos_hive - B.x_pos_hive) ^ 2 + (A.x_pos_hive - B.x_pos_hive) ^ 2)) as distance,\n",
    "                    A.combined_id as frame_id\n",
    "                INTO TEMPORARY TABLE distances\n",
    "                FROM detections_with_combined_id A\n",
    "                INNER JOIN detections_with_combined_id B\n",
    "                ON A.combined_id = B.combined_id\n",
    "                WHERE A.cam_id <> B.cam_id;\n",
    "\n",
    "                SELECT bee_id_a as bee_id, avg(distance) as avg_distance\n",
    "                INTO TEMPORARY TABLE avg_distance\n",
    "                FROM distances\n",
    "                GROUP BY bee_id_a;\n",
    "\n",
    "                SELECT bee_id, avg(count) as avg_neighbors_15\n",
    "                INTO TEMPORARY TABLE avg_neighbors_15\n",
    "                FROM (\n",
    "                    SELECT bee_id_a as bee_id, frame_id, count(bee_id_a)\n",
    "                    FROM distances\n",
    "                    WHERE distance < 15\n",
    "                    GROUP BY frame_id, bee_id\n",
    "                ) q2 GROUP BY bee_id;\n",
    "\n",
    "                SELECT bee_id, avg(count) as avg_neighbors_30\n",
    "                INTO TEMPORARY TABLE avg_neighbors_30\n",
    "                FROM (\n",
    "                    SELECT bee_id_a as bee_id, frame_id, count(bee_id_a) \n",
    "                    FROM distances\n",
    "                    WHERE distance < 30\n",
    "                    GROUP BY frame_id, bee_id\n",
    "                ) q2 GROUP BY bee_id;\n",
    "\n",
    "                SELECT bee_id, avg(count) as avg_neighbors_50\n",
    "                INTO TEMPORARY TABLE avg_neighbors_50\n",
    "                FROM (\n",
    "                    SELECT bee_id_a as bee_id, frame_id, count(bee_id_a) \n",
    "                    FROM distances\n",
    "                    WHERE distance < 50\n",
    "                    GROUP BY frame_id, bee_id\n",
    "                ) q2 GROUP BY bee_id;\n",
    "\n",
    "                SELECT bee_id_a as bee_id, avg(distance) as avg_distance_queen\n",
    "                INTO TEMPORARY TABLE avg_distance_queen\n",
    "                FROM (\n",
    "                    SELECT * FROM distances\n",
    "                    WHERE bee_id_b = 1901\n",
    "                ) query\n",
    "                GROUP BY bee_id_a;\n",
    "                \n",
    "                SELECT q.* \n",
    "                INTO TEMPORARY TABLE num_dances_tmp\n",
    "                FROM (\n",
    "                    SELECT bee_id, MIN(wdd_dances.timestamp_begin) as timestamp, COUNT(*) as num_dances\n",
    "                    FROM wdd_dances_to_bb_detections\n",
    "                    NATURAL JOIN wdd_dances\n",
    "                    WHERE wdd_dances.valid = True\n",
    "                        AND timestamp_begin >= %s\n",
    "                        AND timestamp_end < %s\n",
    "                    GROUP BY bee_id\n",
    "                ) q\n",
    "                INNER JOIN alive_bees_2016 \n",
    "                ON (alive_bees_2016.bee_id = q.bee_id) AND\n",
    "                   (EXTRACT(doy FROM alive_bees_2016.timestamp) = EXTRACT(doy FROM q.timestamp));\n",
    "\n",
    "                SELECT *\n",
    "                INTO TEMPORARY TABLE results_tmp\n",
    "                FROM avg_distance\n",
    "                NATURAL JOIN avg_neighbors_15\n",
    "                NATURAL JOIN avg_neighbors_30\n",
    "                NATURAL JOIN avg_neighbors_50\n",
    "                NATURAL JOIN avg_distance_queen;\n",
    "                \n",
    "                SELECT results_tmp.*, coalesce(num_dances_tmp.num_dances, 0) as num_dances\n",
    "                FROM results_tmp\n",
    "                LEFT JOIN num_dances_tmp\n",
    "                ON results_tmp.bee_id = num_dances_tmp.bee_id\n",
    "                \"\"\"\n",
    "\n",
    "            with conn:\n",
    "                with conn.cursor() as cur:\n",
    "                    cur.execute(self.prefix_query)\n",
    "                    self.data = pd.read_sql_query(\n",
    "                        data_query,\n",
    "                        conn,\n",
    "                        params=(self.ts_begin, self.ts_end, self.confidence),\n",
    "                        coerce_float=False)\n",
    "                    self.sql_features = pd.read_sql_query(\n",
    "                        feature_query,\n",
    "                        conn,\n",
    "                        coerce_float=False,\n",
    "                        params=(self.ts_begin, self.ts_end, self.ts_begin, self.ts_end))\n",
    "                    self.sql_features['Age'] = [\n",
    "                        self.meta.get_age(\n",
    "                            BeesbookID.from_ferwar(k),\n",
    "                            datetime.combine(self.ts_begin.date(),\n",
    "                                             datetime.min.time())).days\n",
    "                        for k in self.sql_features.bee_id.values\n",
    "                    ]\n",
    "\n",
    "        def get_features(self):\n",
    "            assert self.data is not None\n",
    "\n",
    "            df_grouped = self.data.groupby('track_id')\n",
    "\n",
    "            dvs = defaultdict(list)\n",
    "            hds = defaultdict(list)\n",
    "\n",
    "            results = [TrackFeatures.process_group(group, self.min_track_len, self.exit_positions) for name, group in df_grouped]\n",
    "            for res in results:\n",
    "                if res is not None:\n",
    "                    bee_id, dv, hd = res\n",
    "\n",
    "                    dvs[bee_id].append(dv)\n",
    "                    hds[bee_id].append(hd)\n",
    "\n",
    "            self.dvs = dict(\n",
    "                [(k, np.concatenate(v))\n",
    "                 for k, v in sorted(dvs.items(), key=lambda kv: kv[0])])\n",
    "            self.hds = dict(\n",
    "                [(k, np.concatenate(v))\n",
    "                 for k, v in sorted(hds.items(), key=lambda kv: kv[0])])\n",
    "            self.ages = [\n",
    "                self.meta.get_age(\n",
    "                    BeesbookID.from_ferwar(k),\n",
    "                    datetime.combine(self.ts_begin.date(),\n",
    "                                     datetime.min.time())).days\n",
    "                for k, v in sorted(dvs.items(), key=lambda kv: kv[0])\n",
    "            ]\n",
    "            self.ids = [k for k, v in sorted(dvs.items(), key=lambda kv: kv[0])]\n",
    "\n",
    "            movement_threshold = 5\n",
    "\n",
    "            dvs_mean = [\n",
    "                np.mean(v[v > movement_threshold])\n",
    "                for k, v in sorted(self.dvs.items(), key=lambda kv: kv[0])\n",
    "            ]\n",
    "            dvs_median = [\n",
    "                np.median(v[v > movement_threshold])\n",
    "                for k, v in sorted(self.dvs.items(), key=lambda kv: kv[0])\n",
    "            ]\n",
    "\n",
    "            mean_activity = [\n",
    "                np.mean(v > movement_threshold)\n",
    "                for k, v in sorted(self.dvs.items(), key=lambda kv: kv[0])\n",
    "            ]\n",
    "            total_distance = [\n",
    "                np.sum(v[v > movement_threshold])\n",
    "                for k, v in sorted(self.dvs.items(), key=lambda kv: kv[0])\n",
    "            ]\n",
    "            exit_distance = [\n",
    "                np.mean(v)\n",
    "                for k, v in sorted(self.hds.items(), key=lambda kv: kv[0])\n",
    "            ]\n",
    "            time_visible = np.array([\n",
    "                len(v) for k, v in sorted(self.hds.items(), key=lambda kv: kv[0])\n",
    "            ])\n",
    "            time_visible = time_visible / \\\n",
    "                np.max(time_visible[~np.isnan(time_visible)])\n",
    "\n",
    "            self.features = pd.DataFrame(\n",
    "                np.stack([\n",
    "                    self.ids,\n",
    "                    self.ages,\n",
    "                    dvs_mean,\n",
    "                    # dvs_median,\n",
    "                    mean_activity,\n",
    "                    total_distance,\n",
    "                    exit_distance,\n",
    "                    time_visible\n",
    "                ]).T,\n",
    "                columns=[\n",
    "                    'bee_id',\n",
    "                    'Age',\n",
    "                    'Mean movement speed',\n",
    "                    #'Median movement speed',\n",
    "                    'Mean activity',\n",
    "                    'Total movement distance',\n",
    "                    'Mean hive exit distance',\n",
    "                    'Proportion of time visible'\n",
    "                ])\n",
    "\n",
    "            self.features = pd.merge(\n",
    "                self.features, self.sql_features, on=['bee_id', 'Age'])\n",
    "\n",
    "            self.features = self.features.astype(np.float64)\n",
    "            self.features = self.features.loc[np.sum(\n",
    "                np.isnan(self.features.values), axis=1) == 0]\n",
    "        \n",
    "    tfeatures = TrackFeatures(\n",
    "        ts_start + timedelta(days=day),\n",
    "        ts_end + timedelta(days=day),\n",
    "        confidence)\n",
    "\n",
    "    with psycopg2.connect(connect_str) as conn:\n",
    "        tfeatures.get_data(conn)\n",
    "\n",
    "    tfeatures.get_features()\n",
    "    \n",
    "    return tfeatures.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_start = start_date + timedelta(hours=12)\n",
    "dt = timedelta(hours=2)\n",
    "ts_end = ts_start + dt\n",
    "confidence = .95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tnrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((reference_date - start_date).days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "feature_futures = []\n",
    "days_skipped = [0,0]\n",
    "days_skipped[0] = 0\n",
    "days_skipped[1] = 0\n",
    "for day in tqdm(tnrange((reference_date + timedelta(days_after) - start_date).days)): #(reference_date + timedelta(days_after) - start_date).days\n",
    "    try:\n",
    "        feature_futures.append(feature_compute_job(day)) #(reference_date - start_date).days)\n",
    "    except ValueError:\n",
    "        days_skipped[0] += 1\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### getting 2nd set of data for nighttimes\n",
    "ts_start = start_date\n",
    "dt = timedelta(hours=2)\n",
    "ts_end = ts_start + dt\n",
    "confidence = .95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in tqdm(tnrange((reference_date + timedelta(days_after) - start_date).days)): #(reference_date + timedelta(days_after) - start_date).days\n",
    "    try:\n",
    "        feature_futures.append(feature_compute_job(day)) #(reference_date - start_date).days)\n",
    "    except ValueError:\n",
    "        days_skipped[1] += 1\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(days_skipped[0], days_skipped[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [feature_futures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyFeatures:\n",
    "    def __init__(self):\n",
    "        None\n",
    "        \n",
    "tfeatures = DummyFeatures()\n",
    "tfeatures_2 = DummyFeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfeatures.features = pd.DataFrame(feature_futures[0])\n",
    "tfeatures_2.features = pd.DataFrame(feature_futures[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_alive_path = detections_cache_path+'Last_day_alive.csv'\n",
    "last_day_alive_df = pd.read_csv(last_alive_path, \n",
    "                           parse_dates=['max'], \n",
    "                           usecols=['max', 'bee_id'])\n",
    "\n",
    "last_day_alive_df = last_day_alive_df.loc[last_day_alive_df['max'] >= datetime(2016, 7, 20+num_days_to_process)]\n",
    "last_day_alive_df.index = last_day_alive_df['bee_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo:\n",
    "def calculate_x_y_averages(day):\n",
    "#load presence_locations\n",
    "    (csv_name, csv_path) = create_presence_locations_cache_filename(24, day, 60)\n",
    "    print('loading '+csv_path)\n",
    "    presence_locations_df = pd.read_csv(csv_path, dtype=object).iloc[:,1:]\n",
    "    (csv_name, csv_path) = create_presence_cache_filename(24, day, 60)\n",
    "    print('loading '+csv_path)\n",
    "    presence_df = pd.read_csv(csv_path).iloc[:,1:]\n",
    "#calculate trip starts\n",
    "    #Preparing for rolling median\n",
    "    num_nans_to_clean = math.floor(5/2)\n",
    "    #apply copies of the first and last column as offset to prepare for the rolling window\n",
    "    first_col = presence_df.iloc[:, 1:2]\n",
    "    last_col = presence_df.iloc[:, -1:]\n",
    "    presence_df_with_offset = presence_df.iloc[:, 1:]\n",
    "    for i in range(0,num_nans_to_clean):\n",
    "        presence_df_with_offset = pd.concat([first_col, presence_df_with_offset, last_col] ,axis=1)\n",
    "    # Applying rolling median window, to filter out noise in the dataframe\n",
    "    rolled = presence_df_with_offset.rolling(window=5,center=True,axis=1).median()\n",
    "    #clean up to get rid of the NaNs\n",
    "    rolled = rolled.iloc[:, num_nans_to_clean:-num_nans_to_clean]\n",
    "    \n",
    "    trip_starts = calc_trip_starts(rolled, 1440)\n",
    "#create dataframes with avrg x and y coordinates\n",
    "    loc_x_df = presence_locations_df.copy()\n",
    "    loc_x_df.loc[:,:] = 0.0\n",
    "    loc_y_df = presence_locations_df.copy()\n",
    "    loc_y_df.loc[:,:] = 0.0\n",
    "    for i in range(len(trip_starts)):\n",
    "        for j in range(len(trip_starts[i])):\n",
    "            #print(i, trip_starts[i][j],  end=' ')\n",
    "            if presence_locations_df.iat[i,trip_starts[i][j]] != \"0.0\" and \",\" in presence_locations_df.iat[i,trip_starts[i][j]]:\n",
    "                temp=presence_locations_df.iat[i,trip_starts[i][j]]\n",
    "                coordinates=[int(s) for s in temp.replace(\"(\",\"\").replace(\")\",\"\").replace(\",\",\"\").replace(\"-\",\"\").split() if s.isdigit()]\n",
    "                loc_x_df.iat[i,trip_starts[i][j]] = coordinates[0]\n",
    "                loc_y_df.iat[i,trip_starts[i][j]] = coordinates[1]\n",
    "    return(loc_x_df.replace(0,np.NaN).mean(axis=1), loc_y_df.replace(0,np.NaN).mean(axis=1))\n",
    "#attach them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_futures_dead = []\n",
    "feature_futures_alive = []\n",
    "for i in range(2):\n",
    "    for day in tqdm(tnrange((reference_date + timedelta(days_after) - start_date).days - days_skipped[i])): #(reference_date + timedelta(days_after) - start_date).days) , \n",
    "        avrg_x_df, avrg_y_df = calculate_x_y_averages(start_date + timedelta(days=day))\n",
    "        features_df = pd.DataFrame(feature_futures[day+((reference_date + timedelta(days_after) - start_date).days - days_skipped[0])*i])\n",
    "        for bee_id in features_df.bee_id:\n",
    "            features_df.loc[features_df['bee_id'] == bee_id, 'average_x_coordinate'] = avrg_x_df[bee_id]\n",
    "            features_df.loc[features_df['bee_id'] == bee_id, 'average_y_coordinate'] = avrg_y_df[bee_id]\n",
    "        features_df['daytime'] = i\n",
    "        feature_futures_dead.append(features_df)\n",
    "        feature_futures_alive.append(features_df)\n",
    "        for id in last_day_alive_df.index:\n",
    "            last_day = last_day_alive_df.loc[id][1]\n",
    "            if not (last_day > (start_date + timedelta(days=day)) and (last_day <= (start_date + timedelta(days=day) + timedelta(days=num_days_to_process)))):\n",
    "                feature_futures_dead[day+((reference_date + timedelta(days_after) - start_date).days - days_skipped[0])*i] = feature_futures_dead[day+((reference_date + timedelta(days_after) - start_date).days - days_skipped[0])*i][feature_futures_dead[day+((reference_date + timedelta(days_after) - start_date).days - days_skipped[0])*i].bee_id !=id]\n",
    "            elif last_day == (start_date + timedelta(days=day)):\n",
    "                feature_futures_dead[day+((reference_date + timedelta(days_after) - start_date).days - days_skipped[0])*i] = feature_futures_dead[day+((reference_date + timedelta(days_after) - start_date).days - days_skipped[0])*i][feature_futures_dead[day+((reference_date + timedelta(days_after) - start_date).days - days_skipped[0])*i].bee_id !=id]\n",
    "                feature_futures_alive[day+((reference_date + timedelta(days_after) - start_date).days - days_skipped[0])*i] = feature_futures_alive[day+((reference_date + timedelta(days_after) - start_date).days - days_skipped[0])*i][feature_futures_alive[day+((reference_date + timedelta(days_after) - start_date).days - days_skipped[0])*i].bee_id !=id]\n",
    "            else:\n",
    "                feature_futures_alive[day+((reference_date + timedelta(days_after) - start_date).days - days_skipped[0])*i] = feature_futures_alive[day+((reference_date + timedelta(days_after) - start_date).days - days_skipped[0])*i][feature_futures_alive[day+((reference_date + timedelta(days_after) - start_date).days - days_skipped[0])*i].bee_id !=id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfeatures.features = pd.concat([pd.DataFrame(feature_futures_dead[0]), pd.DataFrame(feature_futures_dead[1])], ignore_index=True)\n",
    "tfeatures_2.features = pd.concat([pd.DataFrame(feature_futures_alive[0]), pd.DataFrame(feature_futures_alive[1])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in tqdm(tnrange((reference_date + timedelta(days_after) - start_date).days - 2 - days_skipped[0] + (reference_date + timedelta(days_after) - start_date).days - days_skipped[1])):\n",
    "    tfeatures.features = pd.concat([tfeatures.features, pd.DataFrame(feature_futures_dead[day+2])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in tqdm(tnrange((reference_date + timedelta(days_after) - start_date).days - 2 - days_skipped[0] + (reference_date + timedelta(days_after) - start_date).days - days_skipped[1])):\n",
    "    tfeatures_2.features = pd.concat([tfeatures_2.features, pd.DataFrame(feature_futures_alive[day+2])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfeatures.features.sort_values(by=['bee_id'])\n",
    "#tfeatures.features = tfeatures.features.groupby(['bee_id'], as_index=False, sort=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfeatures_2.features.sort_values(by=['bee_id'])\n",
    "#tfeatures_2.features = tfeatures_2.features.groupby(['bee_id'],as_index=False, sort=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for feature in tfeatures.features.columns:\n",
    "    if feature in ('bee_id', 'Age', 'day'):\n",
    "        continue\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    sns.distplot(tfeatures.features[feature].fillna(0), ax=axes[0])\n",
    "    sns.regplot('Age', feature, tfeatures.features, ax=axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in tfeatures_2.features.columns:\n",
    "    if feature in ('bee_id', 'Age', 'day'):\n",
    "        continue\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    sns.distplot(tfeatures_2.features[feature].fillna(0), ax=axes[0])\n",
    "    sns.regplot('Age', feature, tfeatures_2.features, ax=axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dead_data_df = tfeatures.features.copy()\n",
    "dead_data_df['dies'] = 1\n",
    "dead_data_df = dead_data_df.drop(['bee_id'], axis=1)\n",
    "alive_data_df = tfeatures_2.features.copy()\n",
    "alive_data_df['dies'] = 0\n",
    "alive_data_df = alive_data_df.drop(['bee_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dead_data_df = dead_data_df.sample(frac=1).reset_index(drop=True)\n",
    "alive_data_df = alive_data_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#undersampling part\n",
    "dying_samples = dead_data_df.shape[0]\n",
    "alive_indices = alive_data_df[alive_data_df.dies == 0].index\n",
    "random_ratio = random.uniform(2, 4)\n",
    "random_indices = np.random.choice(alive_indices, dying_samples, replace=False) #int(np.floor(dying_samples*random_ratio))\n",
    "ratio = np.ceil(alive_data_df.shape[0] / dying_samples)\n",
    "\n",
    "trainingsdataset = pd.concat([dead_data_df, alive_data_df.loc[random_indices]], ignore_index=True) #.loc[random_indices]\n",
    "print(trainingsdataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingsdataset = trainingsdataset.sample(frac=1).reset_index(drop=True)\n",
    "trainingsdataset.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = trainingsdataset['dies'].copy()\n",
    "data = trainingsdataset.drop(['dies'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, test_data, training_target, test_target = train_test_split(data, target, test_size=0.25)\n",
    "#test_data = pd.concat([test_data, alive_data_df.loc[2296:].drop(['dies'], axis=1)])\n",
    "#print(test_target.shape)\n",
    "#test_target = pd.concat([test_target, alive_data_df.loc[2296:]['dies']], ignore_index=True)\n",
    "#print(test_target.shape)\n",
    "\n",
    "#sample_weight = np.array([ratio if i == 0 else 1 for i in training_target])\n",
    "#print(sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = Imputer(missing_values='NaN', strategy='median', axis=1)\n",
    "imp = imp.fit(training_data)\n",
    "\n",
    "training_data_imp = imp.transform(training_data)\n",
    "test_data = imp.transform(test_data)\n",
    "\n",
    "dummy = DummyClassifier(strategy='uniform',random_state=0)\n",
    "dct = DecisionTreeClassifier(max_features=3, max_depth=None, min_samples_split=2, random_state=0, class_weight='balanced')\n",
    "etc = ExtraTreesClassifier(n_estimators=1000, max_features=None, max_depth=None, min_samples_split=2, random_state=0, class_weight='balanced_subsample')\n",
    "rfc = RandomForestClassifier(n_estimators=1000, max_features=None, max_depth=None, min_samples_split=2, random_state=0, class_weight='balanced_subsample')\n",
    "ada = AdaBoostClassifier(n_estimators=1000, base_estimator=rfc)\n",
    "#gbr = GradientBoostingRegressor(n_estimators=500, learning_rate=0.1, max_depth=1, random_state=0, loss='ls')\n",
    "gbc = GradientBoostingClassifier(n_estimators= 1000, max_leaf_nodes= 4, max_depth= None, random_state= 2, min_samples_split= 5)\n",
    "nc = NearestCentroid()\n",
    "qda = QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,\n",
    "                              store_covariance=False,\n",
    "                              store_covariances=None, tol=0.0001)\n",
    "\n",
    "dummy=dummy.fit(training_data_imp, training_target)\n",
    "dct=dct.fit(training_data_imp, training_target)\n",
    "etc=etc.fit(training_data_imp, training_target)\n",
    "rfc=rfc.fit(training_data_imp, training_target)\n",
    "ada=ada.fit(training_data_imp, training_target)\n",
    "#gbr=gbr.fit(training_data_imp, training_target)\n",
    "gbc = gbc.fit(training_data_imp, training_target)\n",
    "nc = nc.fit(training_data_imp, training_target)\n",
    "qda = qda.fit(training_data_imp, training_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dummy.score(test_data, test_target))\n",
    "print(dct.score(test_data, test_target))\n",
    "print(etc.score(test_data, test_target))\n",
    "print(rfc.score(test_data, test_target))\n",
    "print(ada.score(test_data, test_target))\n",
    "#print(gbr.score(test_data, test_target))\n",
    "print(gbc.score(test_data, test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dummy.predict(test_data).sum())\n",
    "print(dct.predict(test_data).sum())\n",
    "print(etc.predict(test_data).sum())\n",
    "print(rfc.predict(test_data).sum())\n",
    "print(ada.predict(test_data).sum())\n",
    "#print(gbr.predict(test_data).sum())\n",
    "print(gbc.predict(test_data).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(dummy, test_data, test_target, scoring='f1')\n",
    "print('cross_val_scores:' + str(scores.mean()))\n",
    "print('mean_squared_error:' + str(mean_squared_error(test_target, dummy.predict(test_data))))\n",
    "print('f1_score:' + str(f1_score(test_target, dummy.predict(test_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(dct, test_data, test_target, scoring='f1')\n",
    "print('cross_val_scores:' + str(scores.mean()))\n",
    "print('mean_squared_error:' + str(mean_squared_error(test_target, dct.predict(test_data))))\n",
    "print('f1_score:' + str(f1_score(test_target, dct.predict(test_data))))\n",
    "\n",
    "plt.figure(num=None, figsize=(22, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance')\n",
    "plt.xticks(np.arange(15), ('Age', 'Mean activity','Mean exit distance','Mean movement speed', 'Proportion time visible','Total move distance',  'avrg_x_start', 'avrg_y_start' , 'avg_distance', 'avg_distance_queen', 'avg_neighbors_15', 'avg_neighbors_30', 'avg_neighbors_50', 'daytime', 'num_dances'))\n",
    "plt.plot(dct.feature_importances_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(etc, test_data, test_target, scoring='f1')\n",
    "print('cross_val_scores:' + str(scores.mean()))\n",
    "print('mean_squared_error:' + str(mean_squared_error(test_target, etc.predict(test_data))))\n",
    "print('f1_score:' + str(f1_score(test_target, etc.predict(test_data))))\n",
    "\n",
    "plt.figure(num=None, figsize=(22, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance')\n",
    "plt.xticks(np.arange(15), ('Age', 'Mean activity','Mean exit distance','Mean movement speed', 'Proportion time visible','Total move distance',  'avrg_x_start', 'avrg_y_start' , 'avg_distance', 'avg_distance_queen', 'avg_neighbors_15', 'avg_neighbors_30', 'avg_neighbors_50', 'daytime', 'num_dances'))\n",
    "plt.plot(etc.feature_importances_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(rfc, test_data, test_target, scoring='f1')\n",
    "print('cross_val_scores:' + str(scores.mean()))\n",
    "print('mean_squared_error:' + str(mean_squared_error(test_target, rfc.predict(test_data))))\n",
    "print('f1_score:' + str(f1_score(test_target, rfc.predict(test_data))))\n",
    "\n",
    "plt.figure(num=None, figsize=(22, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance')\n",
    "plt.xticks(np.arange(15), ('Age', 'Mean activity','Mean exit distance','Mean movement speed', 'Proportion time visible','Total move distance',  'avrg_x_start', 'avrg_y_start' , 'avg_distance', 'avg_distance_queen', 'avg_neighbors_15', 'avg_neighbors_30', 'avg_neighbors_50', 'daytime', 'num_dances'))\n",
    "plt.plot(rfc.feature_importances_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(ada, test_data, test_target, scoring='f1')\n",
    "print('cross_val_scores:' + str(scores.mean()))\n",
    "print('mean_squared_error:' + str(mean_squared_error(test_target, ada.predict(test_data))))\n",
    "print('f1_score:' + str(f1_score(test_target, ada.predict(test_data))))\n",
    "\n",
    "plt.figure(num=None, figsize=(22, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance')\n",
    "plt.xticks(np.arange(15), ('Age', 'Mean activity','Mean exit distance','Mean movement speed', 'Proportion time visible','Total move distance',  'avrg_x_start', 'avrg_y_start' , 'avg_distance', 'avg_distance_queen', 'avg_neighbors_15', 'avg_neighbors_30', 'avg_neighbors_50', 'daytime', 'num_dances'))\n",
    "plt.plot(ada.feature_importances_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scores = cross_val_score(gbr, test_data, test_target, scoring='f1')\n",
    "#print('cross_val_scores:' + str(scores.mean()))\n",
    "#print('feature_importances:' + str(gbr.feature_importances_))\n",
    "#print('mean_squared_error:' + str(mean_squared_error(test_target, gbr.predict(test_data))))\n",
    "\n",
    "#plt.figure(num=None, figsize=(22, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "#plt.xlabel('Feature')\n",
    "#plt.ylabel('Importance')\n",
    "#plt.xticks(np.arange(14), ('Age', 'Mean movement speed', 'Mean activity','Total move distance', 'Mean exit distance', 'Proportion time visible', 'avg_distance', 'avg_neighbors_15', 'avg_neighbors_30', 'avg_neighbors_50', 'avg_distance_queen', 'num_dances', 'avrg_x_start', 'avrg_y_start'))\n",
    "#plt.plot(gbr.feature_importances_)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(nc, test_data, test_target, scoring='f1')\n",
    "print('cross_val_scores:' + str(scores.mean()))\n",
    "print('mean_squared_error:' + str(mean_squared_error(test_target, nc.predict(test_data))))\n",
    "print('f1_score:' + str(f1_score(test_target, nc.predict(test_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(qda, test_data, test_target, scoring='f1')\n",
    "print('cross_val_scores:' + str(scores.mean()))\n",
    "print('mean_squared_error:' + str(mean_squared_error(test_target, qda.predict(test_data))))\n",
    "print('f1_score:' + str(f1_score(test_target, qda.predict(test_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(gbc, test_data, test_target, scoring='f1')\n",
    "print('cross_val_scores:' + str(scores.mean()))\n",
    "print('mean_squared_error:' + str(mean_squared_error(test_target, gbc.predict(test_data))))\n",
    "print('f1_score:' + str(f1_score(test_target, gbc.predict(test_data))))\n",
    "\n",
    "plt.figure(num=None, figsize=(22, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance')\n",
    "plt.xticks(np.arange(15), ('Age', 'Mean activity','Mean exit distance','Mean movement speed', 'Proportion time visible','Total move distance',  'avrg_x_start', 'avrg_y_start' , 'avg_distance', 'avg_distance_queen', 'avg_neighbors_15', 'avg_neighbors_30', 'avg_neighbors_50', 'daytime', 'num_dances'))\n",
    "plt.plot(gbc.feature_importances_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_target.shape)\n",
    "print(test_target.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(days_skipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n",
    "sns.distplot(tfeatures.features['Age'].fillna(0), ax=axes[0])\n",
    "sns.distplot(tfeatures_2.features['Age'].fillna(0), ax=axes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
